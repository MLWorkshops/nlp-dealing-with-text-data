{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Text Data Workshop\n",
    "\n",
    "Through examples we will:\n",
    "\n",
    "1.  Retrieve data\n",
    "2.  Ethics checklist\n",
    "3.  Tokenize\n",
    "4.  Normalize\n",
    "5.  Demo: Label data with `doccano`\n",
    "6.  Convert to `spacy` format\n",
    "7.  Extra:  Train a model\n",
    "9.  References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we are using the right pip for the Python kernel\n",
    "# If not using conda, try without the {sys.prefix}/bin part\n",
    "import sys\n",
    "! {sys.prefix}/bin/pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import importlib\n",
    "import spacy\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the language features and model for English for use with spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# This download is actually downloading en_core_web_sm, en is the shortcut name\n",
    "! {sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data\n",
    "\n",
    "Free NY Times recipe data.  Copyright is from the NY Times so please consider this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `requests` library to get a recipe as raw HTML and `BeautifulSoup` to parse through the HTML page to get to content of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://cooking.nytimes.com/recipes/1018442-chicken-soup-from-scratch')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "steps = soup.findAll(\"ol\", {\"class\": \"recipe-steps\"})\n",
    "\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean HTML tags to get raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    \"\"\"Function to clean up the html tags in data.\"\"\"\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    # Remove html tags\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    cleantext = cleantext.replace('\\n', ' ').rstrip().strip()\n",
    "    return cleantext\n",
    "\n",
    "cleansteps = cleanhtml(str(steps[0]))\n",
    "print(cleansteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'sample_data.txt'), 'w') as fptr:\n",
    "    fptr.write(cleansteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics checklist\n",
    "\n",
    "`deon` is a command line tool for creating Data Science ethics checklists.  https://github.com/drivendataorg/deon\n",
    "\n",
    "Run `deon` as follows to create the standard checklist and see the output in the repo folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! deon -o ETHICS.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The portion dealing mainly with data will look like:\n",
    "\n",
    "A. Data Collection\n",
    " - [ ] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
    " - [ ] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
    " - [ ] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    "\n",
    "B. Data Storage\n",
    " - [ ] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    " - [ ] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    " - [ ] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenize text with spaCy\n",
    "\n",
    "Tokenizing is breaking apart a corpus or document into units like words, n-grams or sentences (called sentence tokenization) that make sense for the NLP task at hand.\n",
    "\n",
    "Many libraries perform tokenization like NLTK, [Gensim](https://radimrehurek.com/gensim/utils.html#gensim.utils.tokenize), [spaCy](https://spacy.io/usage/linguistic-features#tokenization).   Oftentimes, the ML practitioner will implement their own tokenizer function.  spaCy does tokenization intelligently, as in the word 'U.K.' _not_ being broken apart into ['U', '.', 'K', '.'], but rather kept intact as it should be in most cases.  Here we'll stick with spaCy for consistency and the intelligent features (utilizing ML behind the scenes).  \n",
    "\n",
    "In spaCy, the tokenizer, going from left to right, performs the following steps:\n",
    "* Splits on whitespace\n",
    "* Checks:\n",
    "  - Does substring match an exception rule?\n",
    "  - Can a prefix, infix or suffix be split off?\n",
    "  \n",
    "Here's an example of how spaCy does tokenization:\n",
    "\n",
    "![spaCy tokenization](https://spacy.io/tokenization-57e618bd79d933c4ccd308b5739062d6.svg)\n",
    "\n",
    "We will re-tokenize later after some more preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If getting OSError with spacy.load('en'), try uncommenting and running the following\n",
    "\n",
    "# importlib.reload(spacy)\n",
    "# ! {sys.prefix}/bin/python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read our data back in\n",
    "with open(os.path.join('data', 'sample_data.txt'), 'r') as fptr:\n",
    "    article = fptr.read()\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 'en_core_web_sm' with it's link 'en' (they are the same thing, i.e. 'en' is the link/shortcut name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = spacy_nlp(article)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize text\n",
    "\n",
    "There is no one way to normalize text and at times it can also require domain knowledge. Normalizing text can include the following.\n",
    "\n",
    "* Convert Unicode charaters to ASCII\n",
    "* Make lowercase\n",
    "* Remove punctuation\n",
    "* Remove stop words\n",
    "* Stemming\n",
    "* Lemmatization\n",
    "\n",
    "\n",
    "Let's do each separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode to ASCII\n",
    "\n",
    "Convert Unicode to ASCII as a form of text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters_numbers = string.ascii_letters + \" .,;'\" + \"0123456789\"\n",
    "n_letters = len(all_letters_numbers)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters_numbers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_article = unicode_to_ascii(article)\n",
    "print(ascii_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with spaCy\n",
    "\n",
    "Lemmatization in linguistics is the process of grouping together the inflected forms of a word so they can be analyzed as a single item, identified by the word's lemma, dictionary form or base word. [[1]](#references)  For instance, \"are, is, being\" becomes \"be\".\n",
    "\n",
    "In spaCy we operate on the Document `doc` from above (which, btw, does much more, actually, than lemmatization).  As a note, Parts of Speech Tagging is used to lemmatize words.\n",
    "\n",
    "A nice diagram of stemming (which we will speak about) and lemmatization (what we _are_ speaking about) is shown here:\n",
    "\n",
    "![stemming vs lemmatization](assets/stem_lemma.png)\n",
    "\n",
    "<div align=\"right\"><a href=\"https://www.quora.com/What-is-difference-between-stemming-and-lemmatization\">Source</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize unless it's a special case, e.g. '-PRON-' replacing 'it'\n",
    "lemmatized_tokens = [token.lemma_ if '-' not in token.lemma_ else token.text for token in doc]\n",
    "print(lemmatized_tokens)\n",
    "\n",
    "# print([token.lemma_ for token in doc]) # to see this pronoun replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words with spaCy\n",
    "\n",
    "In computing, stop words are words which are filtered out before or after processing of natural language data (text). Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these stop words to support phrase search. [[1]](#references)\n",
    "\n",
    "To do this we check the attributes of spaCy Document tokens at:  https://spacy.io/api/token#attributes (look for `is_stop`).\n",
    "\n",
    "High frequency words are then going to be removed, such as:  of, the, and."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_words = [token.text for token in doc if not token.is_stop]\n",
    "print(no_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming with NLTK\n",
    "\n",
    "Stemming is the task of finding the root of a word.  Surprisingly, spaCy does not have Stemmers, so we will turn to the NLTK package for this.  See the how-to at:  http://www.nltk.org/howto/stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmed_tokens = [stemmer.stem(token.text) for token in doc]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put it together\n",
    "\n",
    "**Exercise 1**:  Write one function to convert to ascii and lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text_to_tokens(text):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_tokens = normalize_text_to_tokens(article)\n",
    "print(norm_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'normalized_sample_data.txt'), 'w') as fptr:\n",
    "    fptr.write(' '.join(norm_tokens)\n",
    "               .replace(' .', '.')\n",
    "               .replace(' ,', ',')\n",
    "               .replace(' ;', ';'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to perform the task of NER so we read the normalized data back in to prepare it to be labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'normalized_sample_data.txt'), 'r') as fptr:\n",
    "    article = fptr.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = spacy_nlp(article)\n",
    "\n",
    "for i, token in enumerate(doc.sents):\n",
    "    print('-->Sentence %d: %s' % (i, token.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'normalized_sentences.txt'), 'w') as fptr:\n",
    "    fptr.write('\\n'.join(sent.text for sent in doc.sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo:  Label data with `doccano`\n",
    "\n",
    "`doccano` is an open source text labeling tool.  If you wish to setup on your own, see the instructions at (the docker setup is recommended):  https://github.com/chakki-works/doccano.\n",
    "\n",
    "Once the app is running in the browser, labeling is a simple as importing data (`data/normalized_sentences.txt`), making the label and highlighting the text.\n",
    "\n",
    "![doccano example](assets/doccano.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it is exported as JSON with labels.\n",
    "\n",
    "![doccano export](assets/doccano_export.png)\n",
    "\n",
    "The output has been saved to this repo for your convenience as `data/doccano_annots.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert custom data to spaCy format\n",
    "\n",
    "The doccano export is very similiar to what spaCy expects.  \n",
    "\n",
    "Our doccano format looks like:\n",
    "\n",
    "```json\n",
    "{\"id\": 54, \"text\": \"place the chicken, celery, carrot, onion, parsnip if use, parsley, peppercorn, bay leave and salt in a large soup pot and cover with cold water by 1 inch.\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[109, 117, \"EQUIPMENT\"]]}\n",
    "{\"id\": 55, \"text\": \"bring to a boil over high heat, then immediately reduce the heat to very low.\", \"meta\": {}, \"annotation_approver\": null, \"labels\": []}\n",
    "```\n",
    "\n",
    "The spaCy format looks like:\n",
    "\n",
    "```json\n",
    "TRAIN_DATA = [\n",
    "    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n",
    "    (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOC\"), (18, 24, \"LOC\")]}),\n",
    "]\n",
    "```\n",
    "\n",
    "**Exercise**:  Can you write a script to convert?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "TRAIN_DATA = []\n",
    "\n",
    "# open file and convert here\n",
    "with open('data/doccano_annots.json', 'r') as fptr:\n",
    "    pass\n",
    "\n",
    "\n",
    "print(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra:  Example of training a SpaCy NER model\n",
    "\n",
    "NER or Named Entity Recognition can be achieved with algorithms like RNNs, LSTMs or Bi-LSTMs, for example.  An _entity_ is a person, place or thing upon which the action is placed (e.g. shown here with labels ORG, GPE, MONEY) and is usually spoken about in terms of intents where an _intent_ is the action, want or desire (e.g. \"is looking at buying\").\n",
    "\n",
    "![ner example](assets/ner.png)\n",
    "<div align=\"right\">Image Source:  https://spacy.io/usage/linguistic-features#named-entities-101</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the pipeline and entity recognizer.\n",
    "model = None\n",
    "if model is not None:\n",
    "    spacy_nlp = spacy.load(model)  # load existing spacy model\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    spacy_nlp = spacy.blank('en')  # create blank Language class\n",
    "    print(\"Created blank 'en' model\")\n",
    "\n",
    "if 'ner' not in spacy_nlp.pipe_names:\n",
    "    ner = spacy_nlp.create_pipe('ner')\n",
    "    spacy_nlp.add_pipe(ner)\n",
    "else:\n",
    "    ner = spacy_nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new entity label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new entity labels to entity recognizer\n",
    "LABEL = ['EQUIPMENT']\n",
    "for i in LABEL:\n",
    "    ner.add_label(i)\n",
    "\n",
    "# Inititalizing optimizer for training\n",
    "if model is None:\n",
    "    optimizer = spacy_nlp.begin_training()\n",
    "else:\n",
    "    optimizer = spacy_nlp.entity.create_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model by looping and updating weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# Epochs\n",
    "n_iter = 10\n",
    "\n",
    "# Get names of other pipes to disable them during training to train only NER\n",
    "other_pipes = [pipe for pipe in spacy_nlp.pipe_names if pipe != 'ner']\n",
    "with spacy_nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        batches = minibatch(TRAIN_DATA, size=2)\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            spacy_nlp.update(texts, annotations, sgd=optimizer, drop=0.40,\n",
    "                       losses=losses)\n",
    "        print('Losses', losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Save model\n",
    "output_dir = 'weights'\n",
    "new_model_name = 'en_equipment'\n",
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    spacy_nlp.meta['name'] = new_model_name  # rename model\n",
    "    spacy_nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the saved model\n",
    "\n",
    "# Preprocess text with our function from above\n",
    "test_text = 'Now, fry the vegetables in the sauce pan or skillet, stiring constantly using a spoon, bake in the oven, steam in a large pot or cook in the microwave.'\n",
    "# Normalize with function from above\n",
    "tokens_preprocess = normalize_text_to_tokens(test_text)\n",
    "# Return normalized text to sentence form\n",
    "test_text_processed = ' '.join(tokens_preprocess)\\\n",
    "               .replace(' .', '.')\\\n",
    "               .replace(' ,', ',')\\\n",
    "               .replace(' ;', ';')\n",
    "print(test_text_processed)\n",
    "\n",
    "# Load model and predict on test text\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "doc2 = nlp2(test_text_processed)\n",
    "for ent in doc2.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1.  [NLP Pipeline series by Edward Ma](https://medium.com/@makcedward/nlp-pipeline-word-tokenization-part-1-4b2b547e6a3)\n",
    "2.  [NLP From Scratch: Classifying Names with a Character-Level RNN - on PyTorch Docs](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)\n",
    "3.  [How does Data Noising Help to Improve your NLP Model? by Edward Ma](https://medium.com/towards-artificial-intelligence/how-does-data-noising-help-to-improve-your-nlp-model-480619f9fb10)\n",
    "4.  [Custom Named Entity Recognition Using spaCy by Kaustumbh Jaiswal\n",
    "](https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718)\n",
    "4.  [spaCy pipelines for pre-trained BERT, XLNet and GPT-2 (Use PyTorch-based transformers from within SpaCy)](https://github.com/explosion/spacy-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
